{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dataset of Images\n",
    "\n",
    "Key outcome: create our own image dataset that we can use to train a model to differentiate between whatever images we choose. In this case, I'll be grabbing images of the abominable snowman, and the friendly snowman Olaf from Disney's Frozen for my nieces.     \n",
    "\n",
    "Based on tutorial by Francisco Ingham and Jeremy Howard, inspired by [Adrian Rosebrock](https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get a list of URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and scroll\n",
    "Step 1: Using [Google Images](http://images.google.com) first search for images of Olaf. Repeat this process later to get images of the abominable snowman. The more specific the search, the less pruning you'll need to do later. To make the search more specific, put things you want to exclude, for example for Eurasian wolf images, \"canis lupus lupus\", exclude other variants like so: \"canis lupus lupus\" -dog -arctos -familiaris -baileyi -occidentalis. You can  limit your results to show only photos by clicking on Tools and selecting Photos from the Type dropdown, but Olaf is a character in a cartoon movie, so we'll keep that filter off.   \n",
    "\n",
    "Step 2: Scroll down until we've seen all the images we want to download, or the 'Show more results' button. All the images we scrolled past are now available to download. I want a few more so, I clicked on the button scrolled to the bottom. The max number of images Google Images shows is 700."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download URLs of images\n",
    "\n",
    "Now let's run some Javascript code in the browser to save the URLs of all the images.\n",
    "\n",
    "Step 3: One the page with the image search results, open the javascript console' by pressing CtrlShiftJ in Windows/Linux and CmdOptJ in Mac.\n",
    "\n",
    "Step 4: Paste the JavaScript commands below into the console window to get the urls of each of the images.\n",
    "\n",
    "```javascript\n",
    "urls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);\n",
    "window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n')));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory and upload 1st url file to your server\n",
    "Choose a clear name for your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = 'olaf'\n",
    "file = 'urls_olaf.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/snowman')\n",
    "dest = path/folder\n",
    "dest.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the urls file. Press 'Upload' in your working directory, select your file, then click 'Upload' for each of the displayed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download 1st set of images\n",
    "Let's download our images! fast.ai has a function to download each of these images from their urls. To use it, specify the url filename and the destination folder. The function will download and save all images that can be opened. Images that have an issue being opened will not be saved.\n",
    "\n",
    "You can choose a maximum number of images to be downloaded. We'll choose 300 of the 700 images we selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['olaf','abominable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you have problems downloading, try `max_workers=0` to see exceptions\n",
    "# download_images(path/file, dest, max_pics=20, max_workers=0)\n",
    "download_images(path/file, dest, max_pics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove any images that can't be opened.\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    verify_images(path/c, delete=True, max_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2nd set of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = 'abominable'\n",
    "file = 'urls_abominable.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = Path('data/snowman')\n",
    "dest = path/folder\n",
    "dest.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['olaf','abominable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images(path/file, dest, max_pics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove any images that can't be opened.\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    verify_images(path/c, delete=True, max_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = ImageDataBunch.from_folder(path, train=\".\", valid_pct=0.2,\n",
    "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you already cleaned your data, run this cell instead of the one before\n",
    "# np.random.seed(42)\n",
    "# data = ImageDataBunch.from_csv(\".\", folder=\".\", valid_pct=0.2, csv_labels='cleaned.csv',\n",
    "#         ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at some of the images in our snowman data set.\n",
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes, data.c len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-91095a8bd7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'create_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "learn = create_cnn(data, models.resnet34, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('stage-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "Some of our top losses may not be due to bad performance by our model. There could be images in our data set that shouldn't be. Using the `ImageCleaner` widget from `fastai.widgets` we can prune our top losses, removing photos that don't belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai.widgets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
